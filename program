# Step 1: Import library
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report

# Step 2: Baca file CSV (pakai delimiter=';')
df = pd.read_csv('berita_hoax_real.csv', delimiter=';')
print(df.shape)
print(df.head())

# Step 3: Pra-pemrosesan label dan teks
texts = df['text'].astype(str).values
labels = df['label'].values

# Encode label hoax/real ke angka 0/1
le = LabelEncoder()
labels = le.fit_transform(labels)

# Step 4: Tokenisasi dan Padding
tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
padded = pad_sequences(sequences, maxlen=100, padding='post', truncating='post')

# Step 5: Split data train dan test
X_train, X_test, y_train, y_test = train_test_split(padded, labels, test_size=0.2, random_state=42)

# Step 6: Bangun model CNN
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=16),
    tf.keras.layers.Conv1D(32, 5, activation='relu'),
    tf.keras.layers.GlobalMaxPooling1D(),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

# Step 7: Latih model
history = model.fit(
    X_train, y_train,
    epochs=10,
    validation_data=(X_test, y_test),
    verbose=1
)

# Step 8: Evaluasi model
y_pred = model.predict(X_test)
y_pred_classes = (y_pred > 0.5).astype(int)
print("\nClassification Report:\n")
print(classification_report(y_test, y_pred_classes))
